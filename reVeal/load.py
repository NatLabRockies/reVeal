# -*- coding: utf-8 -*-
"""
load module
"""
from math import isclose
from concurrent.futures import ProcessPoolExecutor, as_completed

import numpy as np
import pandas as pd
import tqdm


def apportion_load_to_regions(load_df, load_value_col, load_year_col, region_weights):
    """
    Apportion aggregate load projections to regions based on a priori input region
    weights.

    Parameters
    ----------
    load_df : pandas.DataFrame
        Load projections dataframe. Should be aggregate totals.
    load_value_col : str
        Name of column containing values of load projections.
    load_year_col : str
        Name of column containing the years associated with each projected load.
    region_weights : dict
        Dictionary indicating weights to use for apportioning load to regions. Keys
        should correspond to region names and values to the proportion of total
        load that should be apportioned to that region. All weights must sum to 1.

    Returns
    -------
    pandas.DataFrame
        Returns a pandas dataframe with the load projections apportioned to regions.
        Dataframe will have a year column (named based on ``load_year_col``), a region
        column (named ``"region"``), and a load projection value column (named based on
        ``load_value_col``.)

    Raises
    ------
    ValueError
        A ValueError will be raised if the input region_weights do not sum to 1.
    """

    weights = np.array(list(region_weights.values()))

    if not isclose(weights.sum(), 1, abs_tol=1e-10, rel_tol=1e-10):
        raise ValueError(
            "Weights of input region_weights must sum to 1. "
            f"Sum of input weights is: {weights.sum()}."
        )

    region_values = load_df[load_value_col].values[:, np.newaxis] * weights
    region_values_df = pd.DataFrame(
        region_values, columns=region_weights.keys(), index=load_df.index
    )

    combined_df = pd.concat([load_df, region_values_df], axis=1)
    combined_df.drop(columns=[load_value_col], inplace=True)

    region_loads_df = combined_df.melt(
        id_vars=[load_year_col], var_name="region", value_name=load_value_col
    )

    return region_loads_df


def simulate_deployment(
    load_projected_in_year, grid_year_df, grid_idx, grid_weights, random_seed
):
    shuffle_df = grid_year_df.sample(
        frac=1,
        replace=False,
        weights=grid_weights,
        random_state=random_seed,
        ignore_index=True,
    )
    shuffle_df["_new_capacity"] = 0.0

    cumulative_developable = shuffle_df["_developable_capacity"].cumsum()
    cumulative_exceeds_total = cumulative_developable > load_projected_in_year
    last_deployed_idx = np.argmax(cumulative_exceeds_total)

    deployed_df = shuffle_df.iloc[0 : last_deployed_idx + 1]

    new_cap_col_idx = deployed_df.columns.get_loc("_new_capacity")
    dev_cap_col_idx = deployed_df.columns.get_loc("_developable_capacity")

    deployed_df.iloc[0:last_deployed_idx, new_cap_col_idx] = deployed_df.iloc[
        0:last_deployed_idx, dev_cap_col_idx
    ]

    total_from_filled_sites = deployed_df["_new_capacity"].sum()

    remaining_capacity = load_projected_in_year - total_from_filled_sites
    deployed_df.iloc[last_deployed_idx, new_cap_col_idx] = remaining_capacity

    total_deployed = deployed_df["_new_capacity"].sum()
    if not isclose(total_deployed, load_projected_in_year):
        raise ValueError("Deployed total is not equal to projected total")

    return deployed_df[[grid_idx, "_new_capacity"]]


def downscale_total(
    grid_df,
    grid_priority_col,
    grid_baseline_load_col,
    baseline_year,
    grid_capacity_col,
    load_df,
    load_value_col,
    load_year_col,
    site_saturation_limit=1,
    priority_power=1,
    n_bootstraps=10_000,
    random_seed=0,
    max_workers=None,
):
    """
    Downscale aggregate load projections to grid based on grid priority column.
    Note that this method uses a random bootstrapping approach to achieve greater
    dispersion of load across multiple grid cells, and the degree of dispersion can
    be tuned manually using input parameters such as ``site_saturation_limit``,
    ``priority_power``, and ``n_bootstraps``.

    Parameters
    ----------
    grid_df : pandas.DataFrame
        Pandas dataframe where each record represents a site to which load projections
        may be downscaled
    grid_priority_col : str
        Name of column in ``grid_df`` to use for prioritizing sites for downscaling
        load.
    grid_baseline_load_col : str
        Name of column in ``grid_df`` with numeric values indicating the baseline, or
        initial, load in each site, corresponding to the ``baseline_year``.
    baseline_year : int
        Year corresponding to the baseline load values in ``grid_baseline_load_col``.
    grid_capacity_col : str
        Name of column in ``grid_df`` indicating the developable capacity of
        load within each site. Note that this value can modified using the
        ``site_saturation_limit`` parameter.
    load_df : pandas.DataFrame
        Dataframe containing aggregate load projections for the area encompassing the
        input ``grid_df`` sites.
    load_value_col : str
        Name of column in ``load_df`` containing projections of load.
    load_year_col : str
        Name of column in ``load_df`` containing year values corresponding to load
        projections.
    site_saturation_limit : float, optional
        Adjustment factor limit the developable capacity of load within each site.
        This value is used to scale the values in the ``grid_capacity_col``. For
        example, to limit the maximum deployed load in each site to half of the
        actual developable load, use ``site_saturation_limit=0.5``. The lower this
        value is set, the greater the degree of dispersion of load  across sites will
        be. The dfault is 1, which leaves the values in the ``grid_capacity_col``
        unmodified.
    priority_power : int, optional
        This factor can be used to exaggerate the influence of the values in
        ``grid_priority_col``, such that higher values have an increased likelihood of
        load deployment and lower values have a decreased likelihood. This effect is
        implemented by raising the values in ``grid_priority_col`` to the specified
        ``priority_power``. As a result, if the input  values in ``grid_priority_col``
        are < 1, setting ``priority_power`` to high values can result in completely
        eliminating lower priority sites from consideration. The default value is 1,
        which leaves the values in ``grid_priority_col`` unmodified. To achieve
        less dispersion and greater clustering of downscaled load in higher priority
        sites, increase this value.
    n_bootstraps : int, optional
        Number of bootstraps to simulate in each projection year. Default is 10,000.
        In general, larger values will produce more stable results, with less chance
        for lower priority sites to receive large amounts of deployed load. However,
        larger values will also cause longer run times.
    random_seed : int, optional
        Random seed to use for reproducible bootstrapping. Default is 0. In general,
        this value does not need to be modified. The exception is if you are interested
        in testing sensitivities and/or producing multiple realizations or scenarios of
        deployment for a given set of values in ``load_priority_col``.
    max_workers : int, optional
        Number of workers to use for bootstrapping. By default None, which uses all
        available workers. In general, this value should only be changed if you are
        running into out-of-memory errors.

    Returns
    -------
    pandas.DataFrame
        Returns DataFrame consisting of load projections downscaled to the grid.
        This dataframe will contain all of the columns from the input ``grid_df``,
        as well as three new columns, including ``year`` (indicating the year
        of the projection) and a "new_" and "total_" load column, named with a suffix
        corresponding to the ``load_value_col``.

    Raises
    ------
    ValueError
        A ValueError will be raised if internal consistency checks for downscaled
        results do not pass.
    """

    grid_df["_weight"] = grid_df[grid_priority_col] ** priority_power
    grid_df[f"total_{load_value_col}"] = grid_df[grid_baseline_load_col].astype(float)
    grid_df[f"new_{load_value_col}"] = float(0.0)
    # note: don't decrement off existing load because developable capacity
    # should already account for exclusions from existing buildings
    grid_df["_developable_capacity"] = (
        grid_df[grid_capacity_col] * site_saturation_limit
    )
    grid_idx = grid_df.index.name
    if grid_idx is None:
        grid_idx = "index"

    grid_year_df = grid_df.reset_index()
    grid_year_df["year"] = baseline_year
    grid_years = [grid_year_df.copy()]

    load_df.sort_values(by=[load_year_col], ascending=True, inplace=True)
    with ProcessPoolExecutor(max_workers=max_workers) as pool:
        for year, year_df in load_df.groupby(by=[load_year_col]):
            grid_year_df["year"] = year[0]

            if len(year_df) > 1:
                raise ValueError(f"Multiple records for load projections year {year}")
            load_projected_in_year = year_df[load_value_col].iloc[0]

            simulations = []
            futures = {}
            grid_year_sub_df = grid_year_df[grid_year_df["_weight"] > 0][
                [grid_idx, "_developable_capacity", "_weight"]
            ]
            with tqdm.tqdm(
                total=n_bootstraps, desc=f"Running bootstraps for year {year}"
            ) as pbar:
                for i in range(0, n_bootstraps):
                    future = pool.submit(
                        simulate_deployment,
                        load_projected_in_year=load_projected_in_year,
                        grid_year_df=grid_year_sub_df,
                        grid_idx=grid_idx,
                        grid_weights="_weight",
                        random_seed=random_seed,
                    )
                    futures[future] = i
                    random_seed += 1

                for future in as_completed(futures):
                    i = futures[future]
                    deployed_df = future.result()
                    simulations.append(deployed_df)
                    pbar.update(1)

            simulations_df = pd.concat(simulations, ignore_index=True)
            means_df = simulations_df.groupby(by=[grid_idx])[["_new_capacity"]].mean()
            means_df["_proportion"] = (
                means_df["_new_capacity"] / means_df["_new_capacity"].sum()
            )
            means_df["_new_calibrated_capacity"] = (
                means_df["_proportion"] * load_projected_in_year
            )
            total_calibrated_deployed = means_df["_new_calibrated_capacity"].sum()
            if not isclose(total_calibrated_deployed, load_projected_in_year):
                raise ValueError("Deployed total is not equal to projected total")

            grid_year_df.set_index(grid_idx, inplace=True)
            grid_year_df.loc[means_df.index, f"new_{load_value_col}"] = means_df[
                "_new_calibrated_capacity"
            ]
            grid_year_df[f"total_{load_value_col}"] += grid_year_df[
                f"new_{load_value_col}"
            ]
            grid_year_df["_developable_capacity"] -= grid_year_df[
                f"new_{load_value_col}"
            ]
            grid_year_df[f"new_{load_value_col}"] = float(0.0)
            grid_year_df.reset_index(inplace=True)

            grid_years.append(grid_year_df.copy())

    grid_projections_df = pd.concat(grid_years, ignore_index=True)
    grid_projections_df.set_index([grid_idx, "year"], inplace=True)
    grid_projections_df.drop(columns=["_developable_capacity"], inplace=True)

    return grid_projections_df


def downscale_regional(
    grid_df,
    grid_priority_col,
    grid_baseline_load_col,
    baseline_year,
    grid_region_col,
    load_df,
    load_value_col,
    load_year_col,
    load_region_col,
):
    # TODO: drop grids with unknown regions
    # TODO: check for validity/consistency of regions across datasets
    # TODO: rename columns for consistency across the input datasets

    return grid_df
